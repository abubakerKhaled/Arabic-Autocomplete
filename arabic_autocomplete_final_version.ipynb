{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DIAlmwRxWep9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sentencepiece as spm\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Paths\n",
        "MODEL_PATH = \"/content/drive/My Drive/arabic_model/model.keras\"\n",
        "TOKENIZER_PATH = \"/content/drive/My Drive/arabic_model/tokenizer.model\"\n",
        "DATASET_PATH = \"/content/drive/My Drive/dataset-nlp\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
        "\n",
        "# Model parameters\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "VOCAB_SIZE = 16000\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "DROPOUT_RATE = 0.3\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 15\n",
        "BEAM_WIDTH = 3"
      ],
      "metadata": {
        "collapsed": true,
        "id": "x_QhNrwDWgwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f20560-ea63-45b5-dd97-65af9f83bf0b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_arabic(text):\n",
        "    text = re.sub(r'[إأآا]', 'ا', text)\n",
        "    text = re.sub(r'[ىي]', 'ي', text)\n",
        "    text = re.sub(r'[ةه]', 'ه', text)\n",
        "    text = re.sub(r'[گك]', 'ك', text)\n",
        "    text = re.sub(r'[\\u064B-\\u065F]', '', text)\n",
        "    text = re.sub('[\\u0640\\\\s]+', ' ', text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "pCKpQq3RWjSi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    sentences = []\n",
        "    for root, _, files in os.walk(DATASET_PATH):\n",
        "        for file in files:\n",
        "            if file.endswith(\".txt\"):\n",
        "                path = os.path.join(root, file)\n",
        "                with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    sentences.extend([normalize_arabic(line.strip())\n",
        "                                    for line in f if line.strip()])\n",
        "    return [s for s in sentences if len(s.split()) >= 3]\n"
      ],
      "metadata": {
        "id": "flwsmKmBWlOq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(sentences):\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    if os.path.exists(TOKENIZER_PATH):\n",
        "        sp.Load(TOKENIZER_PATH)\n",
        "    else:\n",
        "        temp_file = \"temp_arabic.txt\"\n",
        "        with open(temp_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(sentences))\n",
        "\n",
        "        spm.SentencePieceTrainer.train(\n",
        "            input=temp_file,\n",
        "            model_prefix=os.path.join(os.path.dirname(TOKENIZER_PATH), \"tokenizer\"),\n",
        "            vocab_size=VOCAB_SIZE,\n",
        "            character_coverage=1.0,\n",
        "            split_by_unicode_script=True,\n",
        "            pad_id=0,\n",
        "            unk_id=1,\n",
        "            bos_id=2,\n",
        "            eos_id=3\n",
        "        )\n",
        "        os.remove(temp_file)\n",
        "        sp.Load(TOKENIZER_PATH)\n",
        "    return sp"
      ],
      "metadata": {
        "id": "X2SBiBjaWody"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(sentences, tokenizer):\n",
        "    encoded = [tokenizer.EncodeAsIds(s) for s in sentences]\n",
        "\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    for seq in encoded:\n",
        "        for i in range(1, len(seq)):\n",
        "            context = seq[:i][-MAX_SEQUENCE_LENGTH:]\n",
        "            padded = context + [0] * (MAX_SEQUENCE_LENGTH - len(context))\n",
        "            sequences.append(padded)\n",
        "            labels.append(seq[i])\n",
        "\n",
        "    X = np.array(sequences, dtype=np.int32)\n",
        "    y = np.array(labels, dtype=np.int32)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "\n",
        "    return (\n",
        "        train_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE),\n",
        "        val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    )"
      ],
      "metadata": {
        "id": "t3Ek4xt5WqZC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    inputs = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "    x = Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True)(inputs)\n",
        "\n",
        "    # Add use_cudnn=False to avoid the masking error\n",
        "    x = Bidirectional(LSTM(HIDDEN_DIM, return_sequences=True, use_bias=True, dropout=0.2, use_cudnn=False))(x)\n",
        "    x = Bidirectional(LSTM(HIDDEN_DIM, use_bias=True, dropout=0.2, use_cudnn=False))(x)\n",
        "\n",
        "    x = Dense(HIDDEN_DIM, activation='relu')(x)\n",
        "    outputs = Dense(VOCAB_SIZE, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "W7xMkSPlWsdJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_words(text, model, tokenizer, max_predictions=3):\n",
        "    text = normalize_arabic(text)\n",
        "    encoded = tokenizer.EncodeAsIds(text)\n",
        "\n",
        "    # Right padding (to match training data pattern)\n",
        "    padded = encoded[-MAX_SEQUENCE_LENGTH:] + [0] * (MAX_SEQUENCE_LENGTH - len(encoded))\n",
        "    input_seq = np.array([padded])\n",
        "\n",
        "    # Alternative approach: use the model without beam search first to test\n",
        "    try:\n",
        "        preds = model.predict(input_seq, verbose=0)[0]\n",
        "        top_ids = np.argsort(preds)[-BEAM_WIDTH:][::-1]  # Get top predictions and sort them in descending order\n",
        "\n",
        "        results = []\n",
        "        for idx in top_ids:\n",
        "            token = tokenizer.IdToPiece(int(idx))\n",
        "            results.append((token, float(preds[idx])))\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "\n",
        "        try:\n",
        "            tf.config.run_functions_eagerly(True)\n",
        "            preds = model(input_seq, training=False).numpy()[0]\n",
        "            tf.config.run_functions_eagerly(False)\n",
        "\n",
        "            top_ids = np.argsort(preds)[-BEAM_WIDTH:][::-1]\n",
        "            results = []\n",
        "            for idx in top_ids:\n",
        "                token = tokenizer.IdToPiece(int(idx))\n",
        "                results.append((token, float(preds[idx])))\n",
        "\n",
        "            return results\n",
        "        except Exception as e2:\n",
        "            print(f\"Second attempt failed: {e2}\")\n",
        "            # Return a default message\n",
        "            return [((\"prediction failed\"), 0.0)]"
      ],
      "metadata": {
        "id": "JI--pGStWuLi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_prompt(model, tokenizer):\n",
        "    print(\"\\\\nArabic Autocomplete System (type 'exit' to quit)\")\n",
        "    while True:\n",
        "        text = input(\"\\\\nInput text: \")\n",
        "        if text.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        predictions = predict_next_words(text, model, tokenizer)\n",
        "\n",
        "        print(\"\\\\nTop Suggestions:\")\n",
        "        for i, (pred, score) in enumerate(predictions):\n",
        "            print(f\"{i+1}. {text} {pred} (confidence: {score:.4f})\")"
      ],
      "metadata": {
        "id": "yN1PreZxWwIK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load sentences\n",
        "    sentences = load_data()\n",
        "    print(f\"Loaded {len(sentences)} sentences\")\n",
        "\n",
        "    # Get tokenizer\n",
        "    tokenizer = get_tokenizer(sentences)\n",
        "\n",
        "    # Try to load existing model or train new one\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        try:\n",
        "            model = load_model(MODEL_PATH, compile=False)\n",
        "            # Recompile the model with the custom configuration\n",
        "            model.compile(\n",
        "                optimizer=Adam(learning_rate=0.001),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "            print(\"Loaded existing model\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            print(\"Training a new model...\")\n",
        "            train_ds, val_ds = create_dataset(sentences, tokenizer)\n",
        "            model = build_model()\n",
        "\n",
        "            callbacks = [\n",
        "                EarlyStopping(patience=2, restore_best_weights=True),\n",
        "                ModelCheckpoint(MODEL_PATH, save_best_only=True)\n",
        "            ]\n",
        "\n",
        "            model.fit(\n",
        "                train_ds,\n",
        "                validation_data=val_ds,\n",
        "                epochs=EPOCHS,\n",
        "                callbacks=callbacks\n",
        "            )\n",
        "\n",
        "            model.save(MODEL_PATH)\n",
        "            print(f\"Model saved to {MODEL_PATH}\")\n",
        "    else:\n",
        "        train_ds, val_ds = create_dataset(sentences, tokenizer)\n",
        "        model = build_model()\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(patience=2, restore_best_weights=True),\n",
        "            ModelCheckpoint(MODEL_PATH, save_best_only=True)\n",
        "        ]\n",
        "\n",
        "        model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=EPOCHS,\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        model.save(MODEL_PATH)\n",
        "        print(f\"Model saved to {MODEL_PATH}\")\n",
        "\n",
        "    try:\n",
        "        interactive_prompt(model, tokenizer)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during interactive prompt: {e}\")\n",
        "        print(\"Please check your model configuration and try again.\")"
      ],
      "metadata": {
        "id": "ft7tISSwWx3D"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "FBCD1tuPWzkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "398dee2f-b32d-4480-b850-e78c436813bb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 40000 sentences\n",
            "Loaded existing model\n",
            "\\nArabic Autocomplete System (type 'exit' to quit)\n",
            "\\nInput text: exite\n",
            "\\nTop Suggestions:\n",
            "1. exite ، (confidence: 0.0873)\n",
            "2. exite . (confidence: 0.0464)\n",
            "3. exite ▁في (confidence: 0.0374)\n",
            "\\nInput text: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio==3.50.2 --quiet"
      ],
      "metadata": {
        "id": "621yGXeB1Ymx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(sentences):\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.Load(TOKENIZER_PATH)\n",
        "    return sp\n",
        "\n",
        "def predict_next_words(text):\n",
        "    text = normalize_arabic(text)\n",
        "    encoded = tokenizer.EncodeAsIds(text)\n",
        "    padded = encoded[-MAX_SEQUENCE_LENGTH:] + [0] * (MAX_SEQUENCE_LENGTH - len(encoded))\n",
        "    input_seq = np.array([padded])\n",
        "\n",
        "    preds = model.predict(input_seq, verbose=0)[0]\n",
        "    top_ids = np.argsort(preds)[-BEAM_WIDTH:][::-1]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_ids:\n",
        "        token = tokenizer.IdToPiece(int(idx)).replace(\"▁\", \"\").strip()\n",
        "        if token:\n",
        "            results.append(f\"{text.strip()} {token} (confidence: {preds[idx]:.4f})\")\n",
        "    return \"\\n\".join(results)\n",
        "\n",
        "# --- Load model and tokenizer ---\n",
        "model = load_model(MODEL_PATH, compile=False)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "sentences = load_data()\n",
        "tokenizer = get_tokenizer(sentences)\n",
        "\n",
        "# --- Gradio UI ---\n",
        "gr.Interface(\n",
        "    fn=predict_next_words,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Type Arabic text here...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Arabic Autocomplete\",\n",
        "    description=\"Enter the beginning of an Arabic sentence and get word suggestions.\"\n",
        ").launch(share=True)"
      ],
      "metadata": {
        "id": "f0UjvJ_rqiLO",
        "outputId": "ca2c34be-d373-497a-a785-0e71f724180b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMPORTANT: You are using gradio version 3.50.2, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://b5b0d6482cb231c9f2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b5b0d6482cb231c9f2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}